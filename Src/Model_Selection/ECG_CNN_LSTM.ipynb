{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import itertools\n","import time\n","import random\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file \n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mcolors\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import accuracy_score, auc, f1_score, precision_score, recall_score\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import LearningRateScheduler, Callback\n","from tensorflow.data import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mit_train_df = pd.read_csv('../Dataset/mitbih_train.csv', header=None)\n","mit_test_df = pd.read_csv('../Dataset/mitbih_test.csv', header=None)\n","\n","mit_df = pd.concat([mit_train_df, mit_test_df], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mit_df.rename(columns={187: 'class'}, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels = {\n","    0: \"Normal\",\n","    1: \"Artial Premature\",\n","    2: \"Premature ventricular contraction\",\n","    3: \"Fusion of ventricular and normal\",\n","    4: \"Fusion of paced and normal\"\n","}\n","\n","mit_df['label'] = mit_df['class'].map(labels)\n","mit_df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mit_df.to_csv('../Dataset/mitbih.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Basic EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Loading Dataset\n","mit_df = pd.read_csv('../Dataset/mitbih.csv')\n","\n","label_counts = mit_df['label'].value_counts()\n","label_percentages = label_counts / label_counts.sum() * 100\n","\n","ax = label_counts.plot(kind='bar', color=plt.cm.Paired(range(len(label_percentages))), width=0.9)\n","\n","plt.xlabel('Labels')\n","plt.ylabel('Frequency')\n","plt.title('Frequency of Labels in LabelColumn')\n","\n","# Annotate each bar with its percentage and frequency on top\n","for i, (percentage, frequency) in enumerate(zip(label_percentages, label_counts)):\n","    ax.annotate(f'{percentage:.2f}%\\n({frequency})', (i, percentage + 0.5), ha='center', va='bottom', fontsize=8)\n","\n","# Rotate x-axis labels for better visibility\n","plt.xticks(rotation=20, ha='right')\n","plt.show()\n","\n","plt.savefig('data_dist.png', facecolor='w', edgecolor='w', format='png',\n","        transparent=False, bbox_inches='tight', pad_inches=0.1)\n","plt.savefig('data_dist.svg', facecolor='w', edgecolor='w', format='svg',\n","        transparent=False, bbox_inches='tight', pad_inches=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["N = 5\n","samples = [mit_df.loc[mit_df['class'] == cls].sample(N) for cls in range(N)]\n","titles = [labels[cls] for cls in range(5)]\n","\n","with plt.style.context('_classic_test_patch'):\n","    fig, axs = plt.subplots(3, 2, figsize=(20, 7))\n","    for i in range(5):\n","        ax = axs.flat[i]\n","        ax.plot(samples[i].values[:,:-2].transpose())\n","        ax.set_title(titles[i])\n","        #plt.ylabel(\"Amplitude\")\n","\n","    plt.tight_layout()\n","    plt.suptitle(\"ECG Signals\", fontsize=20, y=1.05, weight=\"bold\")\n","    # plt.savefig(f\"signals_per_class.svg\",\n","    #                 format=\"svg\",bbox_inches='tight', pad_inches=0.2)\n","        \n","    # plt.savefig(f\"signals_per_class.png\", \n","    #                 format=\"png\",bbox_inches='tight', pad_inches=0.2) "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","signals = [' '.join(df_mitbih.iloc[i, :-1].apply(str).values) for i in range(df_mitbih.shape[0])]\n","y = df_mitbih.iloc[:, -1].values.tolist()\n","print(len(signals), len(y))\n","\n","print(f'data has {len(set([sig for line in signals for sig in line.split()]))} out of 16 372 411 unique values.')"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ECGDataset:\n","    def __init__(self, df):\n","        self.df = df\n","        # Assuming the last column is the target 'class' and the second last is not needed\n","        self.features = df.iloc[:, :-2].values.astype('float32')\n","        self.labels = df.iloc[:, -1].values.astype('int32')  # Assuming 'class' column is the last one\n","\n","    def generator(self):\n","        for features, label in zip(self.features, self.labels):\n","            yield features, label\n","\n","def create_tf_dataset(df, batch_size=32):\n","    dataset_creator = ECGDataset(df)\n","    return tf.data.Dataset.from_generator(\n","        dataset_creator.generator,\n","        output_types=(tf.float32, tf.int32),\n","        output_shapes=((None,), ())  # Adjust the shape according to your data\n","    ).batch(batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def create_tf_dataset(df, batch_size=32):\n","    dataset_creator = ECGDataset(df)\n","    return tf.data.Dataset.from_generator(\n","        dataset_creator.generator,\n","        output_types=(tf.float32, tf.int32),\n","        output_shapes=((None,), ())\n","    ).batch(batch_size)\n","\n","def get_tf_dataset(phase: str, batch_size: int = 96):\n","    '''\n","    Prepare TensorFlow dataset.\n","    Parameters:\n","        phase: 'train' or 'validation' to specify the dataset phase.\n","        batch_size: Number of samples per batch.\n","    Returns:\n","        A TensorFlow dataset ready for training or validation.\n","    '''\n","    # Assuming config.train_csv_path and config.seed are defined elsewhere\n","    df = pd.read_csv(config.train_csv_path)\n","    train_df, val_df = train_test_split(\n","        df, test_size=0.15, random_state=config.seed, stratify=df['label']\n","    )\n","    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n","    \n","    # Selecting the appropriate dataframe based on the phase\n","    df = train_df if phase == 'train' else val_df\n","    \n","    # Creating the TensorFlow dataset\n","    tf_dataset = create_tf_dataset(df, batch_size)\n","    \n","    return tf_dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define Swish as a custom TensorFlow function\n","def swish(x):\n","    return x * tf.sigmoid(x)\n","\n","# Generate a range of values for plotting\n","x = np.linspace(-10.0, 10.0, 100)\n","x_tensor = tf.constant(x, dtype=tf.float32)\n","\n","# Apply the Swish and ReLU functions\n","swish_out = swish(x_tensor)\n","relu_out = tf.nn.relu(x_tensor)\n","\n","# Plotting\n","plt.title('Swish function')\n","plt.plot(x, swish_out.numpy(), label='Swish')  # Convert TensorFlow tensors to numpy for plotting\n","plt.plot(x, relu_out.numpy(), label='ReLU')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ConvNormPool(tf.keras.Model):\n","    \"\"\"Conv Skip-connection module in TensorFlow\"\"\"\n","    def __init__(self, input_size, hidden_size, kernel_size, norm_type='batchnorm'):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.conv_1 = layers.Conv1D(hidden_size, kernel_size, padding='valid')\n","        self.conv_2 = layers.Conv1D(hidden_size, kernel_size, padding='valid')\n","        self.conv_3 = layers.Conv1D(hidden_size, kernel_size, padding='valid')\n","        self.swish_1 = layers.Activation(swish)\n","        self.swish_2 = layers.Activation(swish)\n","        self.swish_3 = layers.Activation(swish)\n","\n","        if norm_type == 'group':\n","            self.normalization_1 = layers.LayerNormalization(axis=-1) \n","            self.normalization_2 = layers.LayerNormalization(axis=-1)\n","            self.normalization_3 = layers.LayerNormalization(axis=-1)\n","        else:\n","            self.normalization_1 = layers.BatchNormalization()\n","            self.normalization_2 = layers.BatchNormalization()\n","            self.normalization_3 = layers.BatchNormalization()\n","\n","        self.pool = layers.MaxPooling1D(pool_size=2)\n","\n","    def call(self, inputs):\n","        x = self.conv_1(inputs)\n","        x = self.normalization_1(x)\n","        x = self.swish_1(x)\n","        x = tf.pad(x, paddings=[[0, 0], [self.kernel_size - 1, 0], [0, 0]], mode='CONSTANT')\n","\n","        x = self.conv_2(x)\n","        x = self.normalization_2(x)\n","        x = self.swish_2(x)\n","        x = tf.pad(x, paddings=[[0, 0], [self.kernel_size - 1, 0], [0, 0]], mode='CONSTANT')\n","\n","        conv3 = self.conv_3(x)\n","        x = self.normalization_3(conv3)\n","        x = self.swish_3(x)\n","        x = tf.pad(x, paddings=[[0, 0], [self.kernel_size - 1, 0], [0, 0]], mode='CONSTANT')\n","\n","        x = self.pool(x)\n","        return x\n","\n","# Define the Swish function\n","def swish(x):\n","    return x * tf.sigmoid(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CNN(tf.keras.Model):\n","    def __init__(self, input_size=1, hid_size=256, kernel_size=5, num_classes=5):\n","        super().__init__()\n","        \n","        # Define the convolutional blocks\n","        self.conv1 = ConvNormPool(input_size=input_size, hidden_size=hid_size, kernel_size=kernel_size)\n","        self.conv2 = ConvNormPool(input_size=hid_size, hidden_size=hid_size // 2, kernel_size=kernel_size)\n","        self.conv3 = ConvNormPool(input_size=hid_size // 2, hidden_size=hid_size // 4, kernel_size=kernel_size)\n","        \n","        # Define the adaptive average pooling equivalent in TensorFlow\n","        self.avgpool = layers.GlobalAveragePooling1D()\n","        \n","        # Define the fully connected layer\n","        self.fc = layers.Dense(num_classes, activation='softmax')\n","        \n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.avgpool(x)\n","        x = self.fc(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training Stage"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Meter:\n","    def __init__(self, n_classes=5):\n","        self.metrics = {}\n","        self.confusion = np.zeros((n_classes, n_classes))\n","    \n","    def update(self, x, y, loss):\n","        x = np.argmax(x.numpy(), axis=1)  # Assuming x is a TensorFlow tensor\n","        y = y.numpy()  # Assuming y is a TensorFlow tensor\n","        self.metrics['loss'] += loss.numpy()  # Assuming loss is a TensorFlow tensor or value\n","        self.metrics['accuracy'] += accuracy_score(x, y)\n","        self.metrics['f1'] += f1_score(x, y, average='macro')\n","        self.metrics['precision'] += precision_score(x, y, average='macro', zero_division=1)\n","        self.metrics['recall'] += recall_score(x, y, average='macro', zero_division=1)\n","        \n","        self._compute_cm(x, y)\n","        \n","    def _compute_cm(self, x, y):\n","        for prob, target in zip(x, y):\n","            if prob == target:\n","                self.confusion[target][target] += 1\n","            else:\n","                self.confusion[target][prob] += 1\n","    \n","    def init_metrics(self):\n","        self.metrics['loss'] = 0\n","        self.metrics['accuracy'] = 0\n","        self.metrics['f1'] = 0\n","        self.metrics['precision'] = 0\n","        self.metrics['recall'] = 0\n","        \n","    def get_metrics(self):\n","        return self.metrics\n","    \n","    def get_confusion_matrix(self):\n","        return self.confusion\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Trainer:\n","    def __init__(self, net, lr, batch_size, num_epochs):\n","        self.net = net\n","        self.num_epochs = num_epochs\n","        self.criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","        self.scheduler = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=lr, decay_steps=num_epochs, alpha=5e-6)\n","        self.best_loss = float('inf')\n","        self.phases = ['train', 'val']\n","        self.dataloaders = {\n","            phase: get_tf_dataset(phase, batch_size) for phase in self.phases\n","        }\n","        self.train_df_logs = pd.DataFrame()\n","        self.val_df_logs = pd.DataFrame()\n","\n","    @tf.function\n","    def _train_step(self, data, target):\n","        with tf.GradientTape() as tape:\n","            output = self.net(data, training=True)\n","            loss = self.criterion(target, output)\n","        gradients = tape.gradient(loss, self.net.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.net.trainable_variables))\n","        return loss\n","\n","    @tf.function\n","    def _val_step(self, data, target):\n","        output = self.net(data, training=False)\n","        loss = self.criterion(target, output)\n","        return loss\n","    \n","    def _train_epoch(self, phase):\n","        print(f\"{phase} mode | time: {time.strftime('%H:%M:%S')}\")\n","        meter = Meter(n_classes=5)  # Assuming n_classes is defined\n","        meter.init_metrics()\n","\n","        if phase == 'train':\n","            for data, target in self.dataloaders[phase]:\n","                loss = self._train_step(data, target)\n","                meter.update(output.numpy(), target.numpy(), loss.numpy())\n","        else:\n","            for data, target in self.dataloaders[phase]:\n","                loss = self._val_step(data, target)\n","                meter.update(output.numpy(), target.numpy(), loss.numpy())\n","\n","        metrics = meter.get_metrics()\n","        metrics = {k: v / len(self.dataloaders[phase]) for k, v in metrics.items()}\n","        df_logs = pd.DataFrame([metrics])\n","        confusion_matrix = meter.get_confusion_matrix()\n","\n","        # Update logs\n","        if phase == 'train':\n","            self.train_df_logs = pd.concat([self.train_df_logs, df_logs], axis=0)\n","        else:\n","            self.val_df_logs = pd.concat([self.val_df_logs, df_logs], axis=0)\n","\n","        # Show logs\n","        print('{}: {}, {}: {}, {}: {}, {}: {}, {}: {}'\n","              .format(*(x for kv in metrics.items() for x in kv)))\n","\n","        # Plot confusion matrix\n","        fig, ax = plt.subplots(figsize=(5, 5))\n","        cm_ = ax.imshow(confusion_matrix, cmap='hot')\n","        ax.set_title('Confusion matrix', fontsize=15)\n","        plt.colorbar(cm_)\n","        plt.show()\n","\n","        return loss.numpy()\n","    \n","    def run(self):\n","        for epoch in range(self.num_epochs):\n","            print(f\"Epoch {epoch+1}/{self.num_epochs}\")\n","            self.optimizer.learning_rate = self.scheduler(epoch)\n","            self._train_epoch(phase='train')\n","            val_loss = self._train_epoch(phase='val')\n","            \n","            if val_loss < self.best_loss:\n","                self.best_loss = val_loss\n","                print('\\nNew checkpoint\\n')\n","                self.net.save_weights(f\"best_model_epoch{epoch}.tf\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = CNN(num_classes=5, hid_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer = Trainer(net=model, lr=1e-3, batch_size=96, num_epochs=10)\n","trainer.run()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_logs = trainer.train_df_logs\n","train_logs.columns = [\"train_\"+ colname for colname in train_logs.columns]\n","val_logs = trainer.val_df_logs\n","val_logs.columns = [\"val_\"+ colname for colname in val_logs.columns]\n","\n","logs = pd.concat([train_logs,val_logs], axis=1)\n","logs.reset_index(drop=True, inplace=True)\n","logs = logs.loc[:, [\n","    'train_loss', 'val_loss', \n","    'train_accuracy', 'val_accuracy', \n","    'train_f1', 'val_f1',\n","    'train_precision', 'val_precision',\n","    'train_recall', 'val_recall']\n","                                 ]\n","logs.head()\n","logs.to_csv('cnn.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Experiments and Results"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cnn_model = CNN(num_classes=5, hid_size=128)  # Model initialization\n","cnn_model.load_weights(config.cnn_state_path)  # Load the pretrained weights\n","\n","logs = pd.read_csv(config.cnn_logs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["color_scheme = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\n","plot_palettes = [\n","    sns.color_palette(color_scheme, n_colors=2),\n","    sns.color_palette(color_scheme, n_colors=4), \n","    sns.color_palette([color_scheme[0], color_scheme[1], color_scheme[-2], color_scheme[-1]] + color_scheme[2:4], n_colors=6)\n","]\n","\n","figure, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","sns.lineplot(data=logs.loc[:, ['train_loss', 'val_loss']], palette=plot_palettes[0], markers=True, ax=axes[0], linewidth=2.5)\n","axes[0].set_title(\"Model Training Loss\", fontsize=14)\n","axes[0].set_xlabel(\"Training Epoch\", fontsize=14)\n","\n","sns.lineplot(data=logs.loc[:, ['train_accuracy', 'val_accuracy', 'train_f1', 'val_f1']], palette=plot_palettes[1], markers=True, ax=axes[1], linewidth=2.5, legend=\"full\")\n","axes[1].set_title(\"Performance Metrics Through Training\", fontsize=15)\n","axes[1].set_xlabel(\"Training Epoch\", fontsize=14)\n","\n","plt.suptitle('Performance of the CNN Model', fontsize=18)\n","plt.tight_layout()\n","\n","figure.savefig(\"model_performance.png\", format=\"png\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n","figure.savefig(\"model_performance.svg\", format=\"svg\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_lstm = RNNModel(input_size=1, hid_size=64, rnn_type='lstm', bidirectional=True).to(device=config.device)\n","model_lstm.load_weights(config.lstm_state_path)\n","model_lstm.eval()\n","\n","training_logs = pd.read_csv(filepath_or_buffer=config.lstm_logs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["custom_colors = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\n","plot_colors = [\n","    sns.color_palette(custom_colors, n_colors=2),\n","    sns.color_palette(custom_colors, n_colors=4), \n","    sns.color_palette(custom_colors[:2] + custom_colors[-2:] + custom_colors[2:4], n_colors=6)\n","]\n","\n","figure, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","sns.lineplot(data=logs.iloc[:, 0:2], palette=plot_colors[0], markers=True, ax=axes[0], linewidth=2.5)\n","axes[0].set_title(\"Model Training Loss Evolution\", fontsize=14)\n","axes[0].set_xlabel(\"Training Epochs\", fontsize=14)\n","\n","sns.lineplot(data=logs.iloc[:, 2:6], palette=plot_colors[1], markers=True, ax=axes[1], linewidth=2.5, legend=True)\n","axes[1].set_title(\"Training Performance Metrics\", fontsize=15)\n","axes[1].set_xlabel(\"Training Epochs\", fontsize=14)\n","\n","plt.suptitle('Performance Overview: CNN+LSTM Model', fontsize=18)\n","\n","plt.tight_layout()\n","\n","figure.savefig(\"model_evaluation_cnn_lstm.png\", format=\"png\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n","figure.savefig(\"model_evaluation_cnn_lstm.svg\", format=\"svg\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["attention_rnn_model = RNNAttentionModel(input_size=1, hid_size=64, rnn_type='lstm', bidirectional=False)\n","\n","# Load the pretrained model weights into the attention-based RNN model\n","attention_rnn_model.load_weights(config.attn_state_path)\n","\n","# Load the training logs\n","training_logs = pd.read_csv(config.attn_logs)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["unique_colors = ['#C042FF', '#03C576FF', '#FF355A', '#03C5BF', '#96C503', '#C5035B']\n","visualization_palettes = [\n","    sns.color_palette(unique_colors, n_colors=2),\n","    sns.color_palette(unique_colors, n_colors=4), \n","    sns.color_palette(unique_colors[:2] + unique_colors[-2:] + unique_colors[2:4], n_colors=6)\n","]\n","\n","figure, axes = plt.subplots(1, 2, figsize=(12, 4))\n","\n","sns.lineplot(data=logs.iloc[:, 0:2], palette=visualization_palettes[0], markers=True, ax=axes[0], linewidth=2.5)\n","axes[0].set_title(\"Training Loss Over Time\", fontsize=14)\n","axes[0].set_xlabel(\"Training Epoch\", fontsize=14)\n","\n","sns.lineplot(data=logs.iloc[:, 2:6], palette=visualization_palettes[1], markers=True, ax=axes[1], linewidth=2.5, legend=True)\n","axes[1].set_title(\"Key Training Metrics Evolution\", fontsize=15)\n","axes[1].set_xlabel(\"Training Epoch\", fontsize=14)\n","\n","plt.suptitle('Evaluation of CNN+LSTM+Attention Model', fontsize=18)\n","plt.tight_layout()\n","\n","figure.savefig(\"model_attn_evaluation.png\", format=\"png\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n","figure.savefig(\"model_attn_evaluation.svg\", format=\"svg\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Experiments and Results for Test Stage"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_df = pd.read_csv(config.test_csv_path)\n","\n","test_data = tf.data.Dataset.from_tensor_slices((test_df.iloc[:, :-1].values, test_df.iloc[:, -1].values))\n","\n","def preprocess(features, label):\n","    return features, label\n","\n","test_data = test_data.map(preprocess)\n","test_data = test_data.batch(96)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def make_test_stage(dataloader, model, probs=False):\n","    cls_predictions = []\n","    cls_ground_truths = []\n","\n","    for data, cls_target in dataloader:\n","        cls_prediction = model(data, training=False)  # Ensure the model is in inference mode\n","        \n","        # If not returning probabilities, extract class with highest probability\n","        if not probs:\n","            cls_prediction = tf.argmax(cls_prediction, axis=1)\n","        \n","        cls_predictions.append(cls_prediction.numpy())\n","        cls_ground_truths.append(cls_target.numpy())\n","\n","    predictions_cls = np.concatenate(cls_predictions)\n","    ground_truths_cls = np.concatenate(cls_ground_truths)\n","    \n","    return predictions_cls, ground_truths_cls\n","\n","\n","models = [cnn_model, lstm_model, attn_model]"]},{"cell_type":"markdown","metadata":{},"source":["### cnn+lstm model report"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_pred, y_true = make_test_stage(test_dataloader, models[1])\n","y_pred.shape, y_true.shape\n","\n","report = pd.DataFrame(classification_report(y_pred,y_true,output_dict=True)).transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["chosen_colors = ['#00FA9A', '#D2B48C', '#FF69B4']  # Colors selected for the plot\n","\n","percentage_report = report.apply(lambda x: x * 100)\n","\n","figure, plot_ax = plt.subplots(figsize=(13, 4))\n","percentage_report[[\"precision\", \"recall\", \"f1-score\"]].plot(kind='bar',\n","                                                            color=chosen_colors,\n","                                                            legend=True,\n","                                                            fontsize=15,\n","                                                            ax=plot_ax)\n","\n","plot_ax.set_xlabel(\"Classifiers\", fontsize=15)\n","plot_ax.set_ylabel(\"Percent\", fontsize=15)\n","plot_ax.set_xticklabels(\n","    [label for label in list(id_to_label.values()) + [\"average accuracy\", \"macro avg\", \"weighted avg\"]],\n","    rotation=15, fontsize=11)\n","plt.title(\"Performance Metrics for the CNN+LSTM Model\", fontsize=20)\n","\n","for values, patch in zip(report[['precision', 'recall', 'f1-score']].values, plot_ax.patches):\n","    annotation_text = \" \".join([f\"{round(val * 100, 2)}%\" for val in values])\n","\n","    patch_x = patch.get_x() + patch.get_width() - 0.4\n","    patch_y = patch.get_y() + patch.get_height() / 4\n","    plot_ax.annotate(annotation_text, (patch_x, patch_y), fontsize=8, rotation=15, fontweight='bold')\n","\n","figure.savefig(\"model_performance_metrics.png\", format=\"png\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n","figure.savefig(\"model_performance_metrics.svg\", format=\"svg\", pad_inches=0.2, transparent=False, bbox_inches='tight')\n","plt.show()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":29414,"sourceId":37484,"sourceType":"datasetVersion"},{"datasetId":1181233,"sourceId":4190467,"sourceType":"datasetVersion"}],"dockerImageVersionId":30068,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
